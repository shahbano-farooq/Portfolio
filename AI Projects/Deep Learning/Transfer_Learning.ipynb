{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the second part of NN-Training Breakout Activity (LP24)"
      ],
      "metadata": {
        "id": "OwbhVHMduuyy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhzP82fN9Rc"
      },
      "source": [
        "_This notebook contains code from chapter 11 (**Training Deep Neural Networks**) of the texbook._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_30PA-SN9RZ"
      },
      "source": [
        "## Tackling training problems\n",
        "\n",
        "In this activity, you will be experimenting with:\n",
        "\n",
        "*   **Applying transfer learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG77nP-HN9Rd"
      },
      "source": [
        "Run the set-up code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nUcxglCN9Re"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 7)\n",
        "\n",
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3SvpKqqN9Rk"
      },
      "source": [
        "**Let's load the fashion-MNIST dataset for our transfer learning experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RucE0Ba8N9Rk"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMg3riruN9Rt"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning in the context of neural networks refers to a technique where a model trained on one task or dataset is reused or adapted as the starting point for a new model trained on a different but related task or dataset. Instead of starting the training of a neural network model from scratch, transfer learning allows leveraging knowledge gained from solving one problem and applying it to a different but related problem.\n",
        "\n",
        "The basic idea behind transfer learning is that features learned by a model while solving one task can be valuable for solving a different task, especially if the tasks share some underlying structure or patterns.\n",
        "\n",
        "In the figure below, the top model is trained on the large ImageNet dataset and later finetuned for a separate binary classification task. The assumption is that the shapes/patterns learned by the network would be also useful for the second task."
      ],
      "metadata": {
        "id": "OJ49br_ty_xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Transfer Learning](https://www.mdpi.com/sensors/sensors-23-00570/article_deploy/html/images/sensors-23-00570-g001-550.jpg)"
      ],
      "metadata": {
        "id": "DkCbvvImyjMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate the idea summarized above, we will split fashion-MNIST into two tasks:\n",
        "\n",
        "*   Task A: All fashion-MNIST except T-shirts/tops and pullovers (8 classes)\n",
        "*   Task B: Only T-shirts/tops and pullovers (2 classes) and only 200 images for each class\n",
        "\n"
      ],
      "metadata": {
        "id": "zudpTWpdz6-g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWtAH04ON9Rt"
      },
      "source": [
        "* `X_train_A`: all images of all items except for T-shirts/tops and pullovers (classes 0 and 2).\n",
        "* `X_train_B`: a much smaller training set of just the first 200 images of T-shirts/tops and pullovers.\n",
        "\n",
        "The validation set and the test set are also split this way, but without restricting the number of images.\n",
        "\n",
        "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (trousers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots) are somewhat similar to classes in set B (T-shirts/tops and pullovers). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the chapter 14).\n",
        "\n",
        "**Task B:** only 400 images are segmented, we want to train with this small labeled dataset and test with the whole test data in fashion-MNIST (for these 2 classes of course)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing data for Task A and Task B**"
      ],
      "metadata": {
        "id": "lGmpHRsh1Mtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR1p8DThN9Rt"
      },
      "outputs": [],
      "source": [
        "# Split Fashion MNIST into tasks A and B, then train and save\n",
        "#              model A to \"my_model_A\".\n",
        "\n",
        "pos_class_id = class_names.index(\"Pullover\")\n",
        "neg_class_id = class_names.index(\"T-shirt/top\")\n",
        "\n",
        "def split_dataset(X, y):\n",
        "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
        "    y_A = y[~y_for_B]\n",
        "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
        "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
        "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
        "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
        "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
        "\n",
        "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
        "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
        "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
        "\n",
        "# Restricting train data in task B to just 200 samples\n",
        "X_train_B = X_train_B[:200]\n",
        "y_train_B = y_train_B[:200]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining and training a model for task A, saving the trained model in a file**"
      ],
      "metadata": {
        "id": "4s9D48Zp1TYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model_A = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
        "                      validation_data=(X_valid_A, y_valid_A))\n",
        "model_A.save(\"my_model_A\")"
      ],
      "metadata": {
        "id": "St4fSIJJ1I7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can first try to handle task B from scratch without applying transfer learning. Implemented below**"
      ],
      "metadata": {
        "id": "UEUzLKJ41e1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3EN4do0N9Ru"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "model_B = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_B.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
        "                      validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "print('--------------------------')\n",
        "model_B_perf = model_B.evaluate(X_test_B, y_test_B)\n",
        "print('Accuracy on test data:',model_B_perf[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBN1vsNiN9Ru"
      },
      "source": [
        "Note down the performance you obtained. We will now apply transfer learning and compare."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer learning steps:**\n",
        "*   Loading model trained on task A\n",
        "*   Removing the output layer (8 classes)\n",
        "*   Adding a new output layer (2 classes)\n",
        "*   Train this model with data for task B (400 samples only)\n",
        "\n",
        "We will test this model with all the test samples in these 2 classes in fashion-MNIST, not just a subset of the 400."
      ],
      "metadata": {
        "id": "z0aco9bv2AN9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwTFBx4FN9Ru"
      },
      "source": [
        "Note that `model_B_on_A` and `model_A` actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build `model_B_on_A` on top of a *clone* of `model_A`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOorsoQ_N9Ru"
      },
      "outputs": [],
      "source": [
        "# Load model from file\n",
        "model_A = tf.keras.models.load_model(\"my_model_A\")\n",
        "# Make a clone not to destroy model A\n",
        "model_A_clone = tf.keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "# Create model B from model A, just removing the output layer\n",
        "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
        "# Adding a new output layer\n",
        "tf.random.set_seed(42)\n",
        "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6mMUAAKN9Ru"
      },
      "outputs": [],
      "source": [
        "# You often prefer to freeze the first layers, just train the last layer first\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compiling and running training\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
        "                           validation_data=(X_valid_B, y_valid_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT3qfFyBN9Ru"
      },
      "outputs": [],
      "source": [
        "# You may like to reset all layers to trainable again as below\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
        "                           validation_data=(X_valid_B, y_valid_B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKjCvT03N9Ru"
      },
      "source": [
        "So, what's the performance after applying transfer learning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lz74D2CN9Rv"
      },
      "outputs": [],
      "source": [
        "model_B_on_A_perf = model_B_on_A.evaluate(X_test_B, y_test_B)\n",
        "print('Accuracy on test data:',model_B_on_A_perf[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMa_3jjgN9Rv"
      },
      "source": [
        "**Task 1:** Compute the amount of improvement in terms of percentage reduction in error."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final task:**\n",
        "When you finish, go back to the first notebook, put a summary of these experiements at the end of that notebook. It is sufficient that you only upload the first notebook."
      ],
      "metadata": {
        "id": "FyrnHLb_3-Rg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}